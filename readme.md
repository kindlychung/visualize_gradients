# Visualization of the gradients of a neural network using three activation functions

![](https://github.com/kindlychung/visualize_gradients/blob/master/all.png)

This project is a study of effects that different activation functions have on how the gradients change in time.
A simple 7-layer fully connected neural networks was used on a simulated dataset with 3 features and a binary output.
Tested activation functions include:

* Sigmoid
* Tanh
* ReLU



